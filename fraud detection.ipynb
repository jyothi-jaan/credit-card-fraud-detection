{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.float\", \"{:.2f}\".format)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABELS = [\"Normal\", \"Fraud\"]\n\ncount_classes = pd.value_counts(data['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = data[data['Class']==1]\nnormal = data[data['Class']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of Fraudulant transactions: {fraud.shape}\")\nprint(f\"Shape of Non-Fraudulant transactions: {normal.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([fraud.Amount.describe(), normal.Amount.describe()], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([fraud.Time.describe(), normal.Time.describe()], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the time feature\nplt.figure(figsize=(10,8))\n\nplt.subplot(2, 2, 1)\nplt.title('Time Distribution (Seconds)')\n\nsns.distplot(data['Time'], color='blue');\n\n#plot the amount feature\nplt.subplot(2, 2, 2)\nplt.title('Distribution of Amount')\nsns.distplot(data['Amount'],color='blue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data[data.Class == 0].Time.hist(bins=35, color='blue', alpha=0.6)\nplt.figure(figsize=(12, 10))\n\nplt.subplot(2, 2, 1)\ndata[data.Class == 1].Time.hist(bins=35, color='blue', alpha=0.6, label=\"Fraudulant Transaction\")\nplt.legend()\n\nplt.subplot(2, 2, 2)\ndata[data.Class == 0].Time.hist(bins=35, color='blue', alpha=0.6, label=\"Non Fraudulant Transaction\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(20, 20));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap to find any high correlations\n\nplt.figure(figsize=(10,10))\nsns.heatmap(data=data.corr(), cmap=\"seismic\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\n\nX = data.drop('Class', axis=1)\ny = data.Class\n\nX_train_v, X_test, y_train_v, y_test = train_test_split(X, y, \n                                                    test_size=0.3, random_state=42)\nX_train, X_validate, y_train, y_validate = train_test_split(X_train_v, y_train_v, \n                                                            test_size=0.2, random_state=42)\n\nX_train = scalar.fit_transform(X_train)\nX_validate = scalar.transform(X_validate)\nX_test = scalar.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"TRAINING: X_train: {X_train.shape}, y_train: {y_train.shape}\\n{'_'*55}\")\nprint(f\"VALIDATION: X_validate: {X_validate.shape}, y_validate: {y_validate.shape}\\n{'_'*50}\")\nprint(f\"TESTING: X_test: {X_test.shape}, y_test: {y_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(label, prediction, train=True):\n    if train:\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n        \n    elif train==False:\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(label, prediction) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\") \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_d = np.expand_dims(X_train, -1)\nX_test_d = np.expand_dims(X_test, -1)\nX_validate_d = np.expand_dims(X_validate, -1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"TRAINING: X_train: {X_train_d.shape}, y_train: {y_train.shape}\\n{'_'*55}\")\nprint(f\"VALIDATION: X_validate: {X_validate_d.shape}, y_validate: {y_validate.shape}\\n{'_'*50}\")\nprint(f\"TESTING: X_test: {X_test_d.shape}, y_test: {y_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nmodel = Sequential()\nmodel.add(Conv1D(32, 2, activation='relu', input_shape=X_train_d[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(128, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weights = {0:1, 1:5}\nmodel.compile(optimizer=Adam(0.0001), loss='binary_crossentropy', metrics=[keras.metrics.AUC()])\nr = model.fit(X_train_d, y_train, \n              validation_data=(X_validate_d, y_validate),\n              batch_size=500, \n              epochs=epochs, \n#               class_weight=weights\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test_d, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(X_train_d)\ny_test_pred = model.predict(X_test_d)\n\nprint_score(y_train, y_train_pred.round(), train=True)\nprint_score(y_test, y_test_pred.round(), train=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nmodel = Sequential()\nmodel.add(Conv1D(32, 2, activation='relu', input_shape=X_train_d[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n# weights = {0:1, 1:5}\nmodel.compile(optimizer=Adam(0.0001), loss='binary_crossentropy', metrics=[keras.metrics.AUC()])\nr = model.fit(X_train_d, y_train, \n              validation_data=(X_validate_d, y_validate),\n              batch_size=50, \n              epochs=epochs, \n#               class_weight=weights\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test_d, y_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(X_train_d)\ny_test_pred = model.predict(X_test_d)\n\nprint_score(y_train, y_train_pred.round(), train=True)\nprint_score(y_test, y_test_pred.round(), train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Class', axis=1)\ny = data.Class\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Concatinating X_train and y_train\ndf = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\nminority_class = df[df.Class == 1]\nmajority_class = df[df.Class == 0]\n\nmajority_downsample = resample(majority_class, replace=False, \n                               n_samples=minority_class.shape[0], \n                               random_state=42)\n\ndf_2 = pd.concat([majority_downsample, minority_class])\ndf_2.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\nminority_class = df[df.Class == 1]\nmajority_class = df[df.Class == 0]\n\nmajority_downsample = resample(majority_class, replace=False, \n                               n_samples=minority_class.shape[0], \n                               random_state=42)\n\ndf_2 = pd.concat([majority_downsample, minority_class])\ndf_2.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX = df_2.drop('Class', axis=1)\ny = df_2.Class\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.3, random_state=42)\n\nX_train = scaler.fit_transform(X_train)\nX_validate = scaler.transform(X_validate)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.expand_dims(X_train, -1)\nX_test = np.expand_dims(X_test, -1)\nX_validate = np.expand_dims(X_validate, -1)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_validate.shape)\nprint(y_validate.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 150\nmodel = Sequential()\nmodel.add(Conv1D(32, 2, activation='relu', input_shape=X_train[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(128, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n# weights = {0:1, 1:5}\nmodel.compile(optimizer=Adam(0.00001), loss='binary_crossentropy', metrics=[\"accuracy\"])\nr = model.fit(X_train, y_train, \n              validation_data=(X_validate, y_validate),\n              batch_size=50, \n              epochs=epochs, \n#               class_weight=weights\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='val_Loss')\nplt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['accuracy'], label='accuracy')\nplt.plot(r.history['val_accuracy'], label='val_accuracy')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nprint_score(y_train, y_train_pred.round(), train=True)\nprint_score(y_test, y_test_pred.round(), train=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}